% \VignetteIndexEntry{SparseData package}
% \VignetteDepends{Matrix, Biobase, parallel}

\documentclass{article}
%% \usepackage{natbib}
\usepackage{Sweave}
\usepackage[a4paper,margin=1.25in]{geometry}
\usepackage{url}
\usepackage{hyperref}
\newcommand{\SparseData}{\textsf{SparseData} }
\SweaveOpts{keep.source=TRUE} 

\begin{document}

\setkeys{Gin}{width=0.4\textwidth}

\title{Efficient manipulation of sparse data}
\author{Michael Love\\
\texttt{love@molgen.mpg.de}}
\date{\today}

\maketitle

\begin{abstract}
  This package allows for efficient manipulation of experiment data using sparse matrix representations.  The sparse matrix representation used is the \textit{dgCMatrix} class from the \textsf{Matrix} package.  The \textsf{SparseData} package allows users to quickly calculate t-statistics across conditions, in order to provide a ranking of features by their specificity for a given condition.  Other functions are provided for efficient calculation of similarity/distance measures.
\end{abstract}

\tableofcontents

\section{Quick start}

\subsection{Point to count files and feature files}

<<>>=
library(SparseData)
sparse.data.files <- list.files(system.file("extdata",package="SparseData"), 
                                "counts", full=TRUE)
sparse.data.names <- list.files(system.file("extdata",package="SparseData"), 
                                "counts")
feature.file <- list.files(system.file("extdata",package="SparseData"), 
                           "ranges", full=TRUE)
feature.file.name <- list.files(system.file("extdata",package="SparseData"), 
                                "ranges")
# these filenames look like:
sparse.data.names[1]
feature.file.name
@

Note: the \texttt{sparse.data.files} are counts of reads in genomic ranges, 
generated using the BED-Tools suite. The output is sorted to correspond
with a sorted BED file.

\begin{verbatim}
bedtools coverage -abam filename.bam -b sorted_regions.bed -counts |
   sort -k 1,1 -k 2,2n | cut -f 4 > filename.counts
\end{verbatim}

\textbf{Warning:} this sorts the count files alphabetically by chromosome 
and then numerically by the starting base pair. The ranges must also be 
sorted this way, or else the counts will not correspond.

\subsection{Make pheno and feature data}

<<>>=
sparse.data.conditions <- sub(".+(Fetal.+)\\.counts","\\1",sparse.data.names)
phenoData <- AnnotatedDataFrame(data.frame(filename=sparse.data.names, 
                                           conditions=sparse.data.conditions))
feature.data.frame <- read.delim(feature.file, header=FALSE)
featureData <- AnnotatedDataFrame(data.frame(chr=feature.data.frame[,1], 
                                             start=feature.data.frame[,2], 
                                             end=feature.data.frame[,3]))
@ 

\subsection{Build a SparseDataSet}

<<>>=
# threshold incoming count files at 50%
sparse.data.list <- lapply(sparse.data.files, function(filename) {
  sparseThreshold(Matrix(scan(filename,quiet=TRUE), sparse=TRUE), nzr=.5)
})
# bind the sparse columns together as a sparse matrix
sparse.data <- do.call(cBind, sparse.data.list)
sds <- newSparseDataSet(sparseData = sparse.data, 
                        conditions=phenoData$conditions, 
                        featureData=featureData, 
                        phenoData=phenoData)
@

\subsection{Normalize}

<<>>=
logPlusOne <- function(x) log(x + 1)
sparseData(sds) <- applyFunctionSparsely(sparseData(sds), logPlusOne)
norm.mat <- Matrix(diag(1/colMeans(sparseData(sds))),sparse=TRUE)
colnames(norm.mat) <- rownames(pData(sds))
sparseData(sds) <- sparseData(sds) %*% norm.mat
@ 

\subsection{Calculate means and t-statistics}

<<>>=
options(mc.cores=1)
sds <- calculateMeans(sds)
sds <- calculateTStats(sds)
@ 

\subsection{Get regions by decreasing t-statistic}

<<>>=
# the top five features specific to Fetal_Brain
fData(sds)[head(order(-tStats(sds)[["Fetal_Brain"]]),5),]
# the top five global features
fData(sds)[head(order(-means(sds)[["global"]]),5),]
@ 

\section{Input data}

We start by reading in some example data in order to create a \textit{SparseDataSet} object.  The example data are counts of DNase-seq reads, generated by the Roadmap Epigenome Mapping Consortium, in 2000 genomic ranges of 200 bp.  The sample data is listed in \texttt{phenoData} and the ranges are listed in \texttt{featureData}.  The ranges are a subset of nonoverlapping ranges covering the genome, after removing ranges which had more than 25\% overlap with a RepeatMasker region of score greater than 1000.  The data file contains the name including GSM number from the GEO series GSE18927 which is available for download at \url{http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE18927}.

<<>>=
library(SparseData)
sparse.data.files <- list.files(system.file("extdata",package="SparseData"), 
                                "counts", full=TRUE)
sparse.data.names <- list.files(system.file("extdata",package="SparseData"), 
                                "counts")
sparse.data.conditions <- sub(".+(Fetal.+)\\.counts","\\1",sparse.data.names)
phenoData <- AnnotatedDataFrame(data.frame(filename=sparse.data.names, 
                                           conditions=sparse.data.conditions))
feature.file <- list.files(system.file("extdata",package="SparseData"), 
                           "ranges", full=TRUE)
feature.data.frame <- read.delim(feature.file, header=FALSE)
featureData <- AnnotatedDataFrame(data.frame(chr=feature.data.frame[,1], 
                                             start=feature.data.frame[,2], 
                                             end=feature.data.frame[,3]))
@ 

The \texttt{sparse.data.files} are counts of reads in genomic ranges, generated using the BEDTools suite, with calls:

\begin{verbatim}
bedtools coverage -abam filename.bam -b sorted_regions.bed -counts | 
sort -k 1,1 -k 2,2n | cut -f 4 > filename.counts
\end{verbatim}

Alternatively, counts of reads in genomic regions can be found using the python package HTSeq \url{http://www-huber.embl.de/users/anders/HTSeq/doc/overview.html} or the \texttt{summarizeOverlaps} function of the \textsf{GenomicRanges} package.

Here we show one method for constructing a sparse matrix from individual files containing a single column of counts defined over the same ranges.  However, the sparse matrix data can be created in any way described by the \textsf{Matrix} package.

We read in a single column of counts using \texttt{scan}.  These numeric vectors are converted to sparse matrices using \texttt{Matrix} from the \textsf{Matrix} package with argument \texttt{sparse=TRUE}. A function of this package, \texttt{sparseThreshold}, is called, which pushes small values (in absolute value) to zero to achieve a desired nonzero ratio.  A reasonable threshold will vary for different datasets and depending on the memory and time savings desired.  Finally the list of sparse single-column matrices are bound together using \texttt{cBind}, the equivalent function to \texttt{cbind}, defined in the \textsf{Matrix} package.

<<>>=
quantile(scan(sparse.data.files[1],quiet=TRUE),0:10/10)
sparse.data.list <- lapply(sparse.data.files, function(filename) {
  sparseThreshold(Matrix(scan(filename,quiet=TRUE), sparse=TRUE), nzr=.5)
})
sparse.data <- do.call(cBind, sparse.data.list)
@ 

\section{Creating a new \textit{SparseDataSet}}

Now we create a \textit{SparseDataSet} object:

<<>>=
sds <- newSparseDataSet(sparseData = sparse.data, 
                        conditions=phenoData$conditions, 
                        featureData=featureData, 
                        phenoData=phenoData)
@ 

We also include the experiment data:

<<>>=
expData <- new("MIAME",
name="2000 ranges of DNase-seq from Roadmap Epigenome Mapping Consortium",
lab="University of Washington",
contact="rharris1@bcm.tmc.edu",
title="Human Reference Epigenome Mapping Project",
url="http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE18927")
pubMedIds(expData) <- "20944595"
experimentData(sds) <- expData
@ 

Now we have a \textit{SparseDataSet} object with the sparse matrix accessible via \texttt{sparseData}, and the standard \texttt{phenoData} and \texttt{featureData} functions.

<<>>=
head(sparseData(sds),10)
head(pData(sds),3)
head(fData(sds),3)
@ 

The sparsity of the matrix can be calculated using the \texttt{nnzero} function from the \texttt{Matrix} package.

<<>>=
nnzero(sparseData(sds))/prod(dim(sds))
@

While multiplication on sparse matrices works as expected, we implement a function \texttt{applyFunctionSparsely} which allows other operations to be called only on the nonzero elements of the matrix.  Here we demonstrate taking $log(x + 1)$ on the nonzero elements.  Also demonstrated is an example of normalization by dividing each column by its mean (multiplying on the right by a diagonal matrix with elements 1/column-mean). 

<<>>=
logPlusOne <- function(x) log(x + 1)
sparseData(sds) <- applyFunctionSparsely(sparseData(sds), logPlusOne)
norm.mat <- Matrix(diag(1/colMeans(sparseData(sds))),sparse=TRUE)
colnames(norm.mat) <- rownames(pData(sds))
sparseData(sds) <- sparseData(sds) %*% norm.mat
@ 

\section{Calculating statistics}

\texttt{SparseDataSet} methods allow for calculation of means, sum of squares from the means, and t-statistics for all conditions.  The \texttt{calculateMeans} function calculates means and sum of squares for each condition, as well as a ``global'' mean of condition means and a ``global'' sum of condition sum of squares.  The \texttt{calculateMeans} function makes use of the \texttt{mclapply} function in the \textsf{parallel} package, allowing the user to distribute means and sum of squares calculations across multiple cores.  Extra arguments to \texttt{calculateMeans} are passed to \texttt{mclapply}.  The t-statistics which are calculated compare each condition to the mean of all condition means, dividing by a pooled within-condition standard deviation.  More details on the t-statistic calculation is provided later in this vignette and in the man page.

<<>>=
options(mc.cores=1)
sds <- calculateMeans(sds)
sds <- calculateTStats(sds)
@ 

The \texttt{calculateMeans} function avoids recalculating means and sum of squares unless the user sets \texttt{recalc=TRUE}.  This allows for the use of \texttt{combine} to add new samples without having to recalculate means and sum of squares for conditions that do not gain samples, as shown later in the vignette.  Here we see from the order of the list that the recalculated condition mean was added to the end of the list.

<<>>=
names(means(sds))
means(sds)[["Fetal_Brain"]] <- NULL
sds <- calculateMeans(sds)
names(means(sds))
@ 

We can find the features for each condition with the largest t-statistic:

<<>>=
fData(sds)[head(order(-tStats(sds)[["Fetal_Brain"]]),5),]
@ 

Here we plot the features with largest t-statistic, using the function \texttt{image} defined in the \textsf{Matrix} package for sparse matrices.  These are features where the condition of interest has higher values than the mean of all conditions.

\begin{center}
<<fig=TRUE,echo=FALSE>>=
par(mfrow=c(3,1),mar=c(1,1,3,1))
for (cond in levels(conditions(sds))) {
  top.feats <- head(order(-tStats(sds)[[cond]]),5)
  image(t(as.matrix(sparseData(sds)[top.feats, order(conditions(sds))])), main=cond, col=grey(10:0/10), xaxt="n", yaxt="n")
}
@ 
\end{center}

Plotting the features with smallest t-statistic, where the condition of interest has smaller values than the mean of all conditions.

\begin{center}
<<fig=TRUE,echo=FALSE>>=
par(mfrow=c(3,1),mar=c(1,1,3,1))
for (cond in levels(conditions(sds))) {
  bottom.feats <- head(order(tStats(sds)[[cond]]),5)
  image(t(as.matrix(sparseData(sds)[bottom.feats, order(conditions(sds))])), 
        main=cond, col=grey(10:0/10), xaxt="n", yaxt="n")
}
@ 
\end{center}

\section{Calculating correlation and distance matrices}

The package provides a function \texttt{sparseCov} for calculating covariance and correlation matrices without losing the sparsity of the data.  The function calculates the covariance and correlation matrix simultaneously, and returns a list with names \texttt{cov} and \texttt{cor}.  Other functions provided in this package include \texttt{sparseCosine} for the cosine similarity, and \texttt{sparseEuclid} for Euclidean distance.  See the timing vignette for comparisons with dense calculations.

<<>>=
cormat <- sparseCov(sparseData(sds))$cor
@ 

\begin{center}
<<echo=FALSE,fig=TRUE>>=
par(mar=c(1,1,3,1))
image(t(cormat[order(conditions(sds)),rev(order(conditions(sds)))]), 
      col=colorRampPalette(c("red","white","green"))(21), 
      zlim=c(-1,1), xaxt="n", yaxt="n", main="Correlation matrix of data")
@ 
\end{center}

We can calculate the correlation of the data to the matrix of means.

<<>>=
means.matrix <- do.call(cBind, means(sds)[match(
  levels(pData(sds)$condition), names(means(sds)))])
match.cormat <- sparseCov(sparseData(sds), means.matrix)$cor
@ 

\begin{center}
<<fig=TRUE, echo=FALSE>>=
par(mar=c(1,1,3,1))
image(t(match.cormat[order(conditions(sds)),ncol(match.cormat):1]), 
      col=colorRampPalette(c("red","white","green"))(21), 
      zlim=c(-1,1), xaxt="n", yaxt="n", main="Correlation of data to condition means")
@ 
\end{center}

\section{Details on t-statistics}

The t-statistics are calculated comparing a single condition against all samples (including that condition).  Some differences between the t-statistic provided here and the ``typical'' equal-variance t-statistic:

\begin{itemize}
  \item The global mean used is weighted (the mean of condition means), rather than a simple mean of all samples.
  \item The denominator of our t-statistic includes the sum of squared distances to the condition means, rather than to the mean of all samples.  
  \item An offset is included in the denominator (the mean of the pooled standard deviation over all features) to avoid division by zero.  
\end{itemize}
    
The resulting t-statistics are closely related in rank to the typical equal-variance t-statistics.  Below we provide a formula, using the notation of Tibshirani, R., Hastie, T., Narasimhan, B. \& Chu, G. ``Diagnosis of multiple cancer types by shrunken centroids of gene expression''. Proceedings of the National Academy of Sciences 99, 6567-6572 (2002).  For $n_k$ samples in condition $k$, $n$ total samples, $K$ conditions, weighted mean $\mu_w$ and sum of squares of samples to their condition means $SSE^*$, the t-statistic provided by \texttt{calculateTStats} is defined by:

  $$ s = \sqrt{SSE^* / (n - K)} $$
  $$ t_k = \frac{1}{\sqrt{(1/n_k + 1/n)}} \left( \frac{\mu_k - \mu_w}{s + \textrm{offset}} \right) $$

For sum of squares for condition $k$ $SSE_{k}$, the mean of all samples $\mu$, and sum of squares of samples to the mean of all samples $SSE$, the equal-variance t-statistic is:

  $$ s = \sqrt{(SSE_{k} + SSE) / (n_k + n - 2)} $$
  $$ t_k = \frac{1}{\sqrt{(1/n_k + 1/n)}} \left( \frac{\mu_k - \mu}{s} \right) $$

A scatter plot of \texttt{tStats} against the equal-variance t-statistic calculated by \texttt{t.test}.

\begin{center}
<<fig=TRUE>>=
sim.sds <- simulateSparseDataSet(200, c(50, 50, 50), nzg = 0.5, nzs = 0.5)
sim.sds <- calculateMeans(sim.sds, quiet=TRUE)
sim.sds <- calculateTStats(sim.sds, quiet=TRUE)
equalvar.t.stats <- sapply(1:nrow(sim.sds), function(i) t.test(
  x=sparseData(sim.sds)[i,pData(sim.sds)$condition == "c1"], 
  y=sparseData(sim.sds)[i,], var.equal=TRUE)$statistic)
plot(equalvar.t.stats, tStats(sim.sds)[["c1"]], 
     xlab="equal-variance t-statistics",ylab="tStats",
     main="tStats vs. equal-variance t-statistics")
abline(0,1)
@ 
\end{center}

\section{Combining \textit{SparseDataSet} objects}

Methods inherited from \textit{eSet} should work as expected, including indexing (because the default for \textit{eSet} is to set \texttt{drop = FALSE}, preserving the sparsity of the matrix stored in assayData).  We define a method \texttt{combine} for the class \texttt{SparseDataSet} and the class \texttt{dgCMatrix}.  Row slicing of column sparse matrices can be very slow if the matrix is large, therefore we only allow combination of objects which have the same feature names in the same order, and we also require that the sample names of the two objects have no intersection.

Here we demonstrate combination of two \texttt{SparseDataSet} objects.  We add unique sample names to the object \texttt{y}.

<<>>=
x <- simulateSparseDataSet(10,c(2,2,2))
y <- simulateSparseDataSet(10,c(2,2))
sampleNames(y) <- paste("sample",(ncol(x) + 1:ncol(y)),sep="")
pData(y)$sampleID <- sampleNames(y)
z <- combine(x,y)
pData(z)
sparseData(z)
@ 

Furthermore, the combine method will check to see if there are shared conditions between the two \textit{SparseDataSet} objects.  If so, the means and sum of squares for these will be removed, as they need to be recalculated.  The t-statistics are also removed as the global mean and global sum of squares have changed.

<<>>=
x <- calculateMeans(x)
x <- calculateTStats(x)
y <- calculateMeans(y)
y <- calculateTStats(y)
z <- combine(x,y)
names(means(z))
names(tStats(z))
z <- calculateMeans(z)
z <- calculateTStats(z)
names(means(z))
names(tStats(z))
@ 

\section{Session info}

<<>>=
sessionInfo()
@ 

\end{document}
